{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Pipe line for Analyzing US immigration data\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "Build a data pipeline to analyze the US Immigration data to provide valuable insights into the immigration patters. \n",
    "Few of the many questions that can be answered are  \n",
    " 1. Most preferred Airline\n",
    " 2. % of people traveeling in each of the Visa Categogry\n",
    " 3. Avg Duration of stay in each of the Visa Category\n",
    " 4. Most busy Airport\n",
    " 5. Number of People potentially over staying than allowed as per the Visa Categogy\n",
    " 6. Weather in which travel is maximum and minimum\n",
    " 7. Most travelled destination\n",
    " 8. Top country from where tourists are coming\n",
    " \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Analyze the data to understand the relation ship of Entities and their attributes\n",
    "\n",
    "#### Describe and Gather Data \n",
    "1. I94 Immigration Data: This data comes from the US National Tourism and Trade Office.  \n",
    "   A data dictionary is included in the workspace.Data comes from https://travel.trade.gov/.  \n",
    "2. World Temperature Data: This dataset came from Kaggle. URL for data source is https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data\n",
    "3. U.S. City Demographic Data: This data comes from OpenSoft.  \n",
    "   URL for this datasource is https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/\n",
    "4. Airport Code Table: This is a simple table of airport codes and corresponding cities.  \n",
    "   URL for data source is https://datahub.io/core/airport-codes#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "        config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "        .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "|5748520.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     F|  null|     DL|9.495645143E10|00040|      B1|\n",
      "|5748521.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  28.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1988.0|10292016|     M|  null|     DL|9.495638813E10|00040|      B1|\n",
      "|5748522.0|2016.0|   4.0| 245.0| 464.0|    HHW|20574.0|    1.0|     HI|20579.0|  57.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1959.0|10292016|     M|  null|     NZ|9.498180283E10|00010|      B2|\n",
      "|5748523.0|2016.0|   4.0| 245.0| 464.0|    HHW|20574.0|    1.0|     HI|20586.0|  66.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1950.0|10292016|     F|  null|     NZ|9.497968993E10|00010|      B2|\n",
      "|5748524.0|2016.0|   4.0| 245.0| 464.0|    HHW|20574.0|    1.0|     HI|20586.0|  41.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1975.0|10292016|     F|  null|     NZ|9.497974673E10|00010|      B2|\n",
      "|5748525.0|2016.0|   4.0| 245.0| 464.0|    HOU|20574.0|    1.0|     FL|20581.0|  27.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1989.0|10292016|     M|  null|     NZ|9.497324663E10|00028|      B2|\n",
      "|5748526.0|2016.0|   4.0| 245.0| 464.0|    LOS|20574.0|    1.0|     CA|20581.0|  26.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1990.0|10292016|     F|  null|     NZ|9.501354793E10|00002|      B2|\n",
      "|5748527.0|2016.0|   4.0| 245.0| 504.0|    NEW|20574.0|    1.0|     MA|20576.0|  44.0|    2.0|  1.0|20160430|     GUZ| null|      G|      O|   null|      M| 1972.0|10292016|     M|  null|     UA|9.493828593E10|01215|      B2|\n",
      "|5748528.0|2016.0|   4.0| 245.0| 504.0|    LOS|20574.0|    1.0|   null|20575.0|  39.0|    2.0|  1.0|20160430|     GUZ| null|      G|      O|   null|      M| 1977.0|10292016|     M|  null|     CM|9.501810463E10|00472|      B2|\n",
      "|5748529.0|2016.0|   4.0| 245.0| 504.0|    WAS|20574.0|    1.0|     VA|20596.0|  38.0|    2.0|  1.0|20160430|     PNM| null|      G|      O|   null|      M| 1978.0|10292016|     M|  null|     CM|9.492489983E10|00488|      B2|\n",
      "|5748530.0|2016.0|   4.0| 245.0| 504.0|    LOS|20574.0|    1.0|     CA|20577.0|  56.0|    2.0|  1.0|20160430|     PNM| null|      G|      O|   null|      M| 1960.0|10292016|     F|  null|     CM|9.492648103E10|00302|      B2|\n",
      "|5748531.0|2016.0|   4.0| 245.0| 504.0|    LOS|20574.0|    1.0|     CA|20577.0|  38.0|    2.0|  1.0|20160430|     PNM| null|      G|      O|   null|      M| 1978.0|10282016|     M|  null|     CM|9.492629303E10|00302|      B2|\n",
      "|5748532.0|2016.0|   4.0| 245.0| 504.0|    MIA|20574.0|    1.0|     FL|20581.0|  53.0|    2.0|  1.0|20160430|     PNM| null|      G|      O|   null|      M| 1963.0|10292016|     F|  null|     CM|9.500640513E10|00430|      B2|\n",
      "|5748534.0|2016.0|   4.0| 245.0| 528.0|    SFR|20574.0|    1.0|     CA|   null|  84.0|    2.0|  1.0|20160430|     HNK| null|      G|   null|   null|   null| 1932.0|10282016|     F|  null|     CX|9.492476223E10|00872|      B2|\n",
      "|5748876.0|2016.0|   4.0| 245.0| 582.0|    HOU|20574.0|    1.0|     TX|20583.0|  43.0|    1.0|  1.0|20160430|     GUZ| null|      G|      O|   null|      M| 1973.0|10292016|     M|  null|     UA|9.499463063E10|05574|      B1|\n",
      "|5748877.0|2016.0|   4.0| 245.0| 582.0|    HOU|20574.0|    1.0|     TX|20583.0|  30.0|    1.0|  1.0|20160430|     GUZ| null|      G|      O|   null|      M| 1986.0|10292016|     F|  null|     UA|9.499447663E10|05574|      B1|\n",
      "|5748881.0|2016.0|   4.0| 245.0| 582.0|    LOS|20574.0|    1.0|     CA|20575.0|  34.0|    2.0|  1.0|20160430|     SHG| null|      G|      O|   null|      M| 1982.0|10292016|     M|  null|     AM|9.496770903E10|00646|      B2|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "initial_num_records = df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialnumber of records in i94 dataset from Apr 16 is : 3096313\n"
     ]
    }
   ],
   "source": [
    "print(\"Initialnumber of records in i94 dataset from Apr 16 is : {}\".format(initial_num_records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#define function to count missing values in each column\n",
    "#Code referenced from https://stackoverflow.com/questions/44627386/how-to-find-count-of-null-and-nan-values-for-each-column-in-a-pyspark-dataframe\n",
    "def count_missings(spark_df,sort=True):\n",
    "    \"\"\"\n",
    "    Counts number of nulls and nans in each column\n",
    "    \"\"\"\n",
    "    df = spark_df.select([F.count(F.when(F.isnan(c) | F.isnull(c), c)).alias(c) for (c,c_type) in spark_df.dtypes ]).toPandas()\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print(\"There are no any missing values!\")\n",
    "        return None\n",
    "\n",
    "    if sort:\n",
    "        return df.rename(index={0: 'count'}).T.sort_values(\"count\",ascending=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>entdepu</th>\n",
       "      <td>3095921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>occup</th>\n",
       "      <td>3088187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insnum</th>\n",
       "      <td>2982605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visapost</th>\n",
       "      <td>1881250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>414269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94addr</th>\n",
       "      <td>152592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depdate</th>\n",
       "      <td>142457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matflag</th>\n",
       "      <td>138429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entdepd</th>\n",
       "      <td>138429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airline</th>\n",
       "      <td>83627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fltno</th>\n",
       "      <td>19549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94bir</th>\n",
       "      <td>802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biryear</th>\n",
       "      <td>802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dtaddto</th>\n",
       "      <td>477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94mode</th>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entdepa</th>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dtadfile</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admnum</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cicid</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94yr</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94visa</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrdate</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94port</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94res</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94cit</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94mon</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visatype</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count\n",
       "entdepu   3095921\n",
       "occup     3088187\n",
       "insnum    2982605\n",
       "visapost  1881250\n",
       "gender     414269\n",
       "i94addr    152592\n",
       "depdate    142457\n",
       "matflag    138429\n",
       "entdepd    138429\n",
       "airline     83627\n",
       "fltno       19549\n",
       "i94bir        802\n",
       "biryear       802\n",
       "dtaddto       477\n",
       "i94mode       239\n",
       "entdepa       238\n",
       "dtadfile        1\n",
       "admnum          0\n",
       "cicid           0\n",
       "i94yr           0\n",
       "count           0\n",
       "i94visa         0\n",
       "arrdate         0\n",
       "i94port         0\n",
       "i94res          0\n",
       "i94cit          0\n",
       "i94mon          0\n",
       "visatype        0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_missings(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create datetime column from original timestamp column\n",
    "df_spark_new = df_spark.withColumn('depdate', F.date_format(df_spark.depdate.cast(dataType=T.TimestampType()), \"yyyy-MM-dd\"))\\\n",
    "                       .withColumn('dtaddto', F.date_format(df_spark.dtaddto.cast(dataType=T.TimestampType()), \"yyyy-MM-dd\"))\\\n",
    "                       .withColumn('biryear', df_spark.biryear.cast(dataType=T.IntegerType()))\\\n",
    "                       .withColumn('i94yr', df_spark.i94yr.cast(dataType=T.IntegerType()))\\\n",
    "                       .withColumn('i94mon', df_spark.i94mon.cast(dataType=T.IntegerType()))\\\n",
    "                       .withColumn('arrdate', F.date_format(df_spark.arrdate.cast(dataType=T.TimestampType()), \"yyyy-MM-dd\"))\\\n",
    "                       .withColumn('dtadfile', F.to_date('dtadfile','yyyyMMdd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: integer (nullable = true)\n",
      " |-- i94mon: integer (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: string (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: string (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: date (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: integer (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+------+------+-------+----------+-------+-------+----------+------+-------+-----+----------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid|i94yr|i94mon|i94cit|i94res|i94port|   arrdate|i94mode|i94addr|   depdate|i94bir|i94visa|count|  dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+-----+------+------+------+-------+----------+-------+-------+----------+------+-------+-----+----------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0| 2016|     4| 245.0| 438.0|    LOS|1970-01-01|    1.0|     CA|1970-01-01|  40.0|    1.0|  1.0|2016-04-30|     SYD| null|      G|      O|   null|      M|   1976|   null|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0| 2016|     4| 245.0| 438.0|    LOS|1970-01-01|    1.0|     NV|1970-01-01|  32.0|    1.0|  1.0|2016-04-30|     SYD| null|      G|      O|   null|      M|   1984|   null|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0| 2016|     4| 245.0| 438.0|    LOS|1970-01-01|    1.0|     WA|1970-01-01|  29.0|    1.0|  1.0|2016-04-30|     SYD| null|      G|      O|   null|      M|   1987|   null|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "|5748520.0| 2016|     4| 245.0| 438.0|    LOS|1970-01-01|    1.0|     WA|1970-01-01|  29.0|    1.0|  1.0|2016-04-30|     SYD| null|      G|      O|   null|      M|   1987|   null|     F|  null|     DL|9.495645143E10|00040|      B1|\n",
      "|5748521.0| 2016|     4| 245.0| 438.0|    LOS|1970-01-01|    1.0|     WA|1970-01-01|  28.0|    1.0|  1.0|2016-04-30|     SYD| null|      G|      O|   null|      M|   1988|   null|     M|  null|     DL|9.495638813E10|00040|      B1|\n",
      "|5748522.0| 2016|     4| 245.0| 464.0|    HHW|1970-01-01|    1.0|     HI|1970-01-01|  57.0|    2.0|  1.0|2016-04-30|     ACK| null|      G|      O|   null|      M|   1959|   null|     M|  null|     NZ|9.498180283E10|00010|      B2|\n",
      "|5748523.0| 2016|     4| 245.0| 464.0|    HHW|1970-01-01|    1.0|     HI|1970-01-01|  66.0|    2.0|  1.0|2016-04-30|     ACK| null|      G|      O|   null|      M|   1950|   null|     F|  null|     NZ|9.497968993E10|00010|      B2|\n",
      "|5748524.0| 2016|     4| 245.0| 464.0|    HHW|1970-01-01|    1.0|     HI|1970-01-01|  41.0|    2.0|  1.0|2016-04-30|     ACK| null|      G|      O|   null|      M|   1975|   null|     F|  null|     NZ|9.497974673E10|00010|      B2|\n",
      "|5748525.0| 2016|     4| 245.0| 464.0|    HOU|1970-01-01|    1.0|     FL|1970-01-01|  27.0|    2.0|  1.0|2016-04-30|     ACK| null|      G|      O|   null|      M|   1989|   null|     M|  null|     NZ|9.497324663E10|00028|      B2|\n",
      "|5748526.0| 2016|     4| 245.0| 464.0|    LOS|1970-01-01|    1.0|     CA|1970-01-01|  26.0|    2.0|  1.0|2016-04-30|     ACK| null|      G|      O|   null|      M|   1990|   null|     F|  null|     NZ|9.501354793E10|00002|      B2|\n",
      "|5748527.0| 2016|     4| 245.0| 504.0|    NEW|1970-01-01|    1.0|     MA|1970-01-01|  44.0|    2.0|  1.0|2016-04-30|     GUZ| null|      G|      O|   null|      M|   1972|   null|     M|  null|     UA|9.493828593E10|01215|      B2|\n",
      "|5748528.0| 2016|     4| 245.0| 504.0|    LOS|1970-01-01|    1.0|   null|1970-01-01|  39.0|    2.0|  1.0|2016-04-30|     GUZ| null|      G|      O|   null|      M|   1977|   null|     M|  null|     CM|9.501810463E10|00472|      B2|\n",
      "|5748529.0| 2016|     4| 245.0| 504.0|    WAS|1970-01-01|    1.0|     VA|1970-01-01|  38.0|    2.0|  1.0|2016-04-30|     PNM| null|      G|      O|   null|      M|   1978|   null|     M|  null|     CM|9.492489983E10|00488|      B2|\n",
      "|5748530.0| 2016|     4| 245.0| 504.0|    LOS|1970-01-01|    1.0|     CA|1970-01-01|  56.0|    2.0|  1.0|2016-04-30|     PNM| null|      G|      O|   null|      M|   1960|   null|     F|  null|     CM|9.492648103E10|00302|      B2|\n",
      "|5748531.0| 2016|     4| 245.0| 504.0|    LOS|1970-01-01|    1.0|     CA|1970-01-01|  38.0|    2.0|  1.0|2016-04-30|     PNM| null|      G|      O|   null|      M|   1978|   null|     M|  null|     CM|9.492629303E10|00302|      B2|\n",
      "|5748532.0| 2016|     4| 245.0| 504.0|    MIA|1970-01-01|    1.0|     FL|1970-01-01|  53.0|    2.0|  1.0|2016-04-30|     PNM| null|      G|      O|   null|      M|   1963|   null|     F|  null|     CM|9.500640513E10|00430|      B2|\n",
      "|5748534.0| 2016|     4| 245.0| 528.0|    SFR|1970-01-01|    1.0|     CA|      null|  84.0|    2.0|  1.0|2016-04-30|     HNK| null|      G|   null|   null|   null|   1932|   null|     F|  null|     CX|9.492476223E10|00872|      B2|\n",
      "|5748876.0| 2016|     4| 245.0| 582.0|    HOU|1970-01-01|    1.0|     TX|1970-01-01|  43.0|    1.0|  1.0|2016-04-30|     GUZ| null|      G|      O|   null|      M|   1973|   null|     M|  null|     UA|9.499463063E10|05574|      B1|\n",
      "|5748877.0| 2016|     4| 245.0| 582.0|    HOU|1970-01-01|    1.0|     TX|1970-01-01|  30.0|    1.0|  1.0|2016-04-30|     GUZ| null|      G|      O|   null|      M|   1986|   null|     F|  null|     UA|9.499447663E10|05574|      B1|\n",
      "|5748881.0| 2016|     4| 245.0| 582.0|    LOS|1970-01-01|    1.0|     CA|1970-01-01|  34.0|    2.0|  1.0|2016-04-30|     SHG| null|      G|      O|   null|      M|   1982|   null|     M|  null|     AM|9.496770903E10|00646|      B2|\n",
      "+---------+-----+------+------+------+-------+----------+-------+-------+----------+------+-------+-----+----------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min arrdate i94 dataset from Apr 16 is : 1970-01-01\n",
      "max arrdate i94 dataset from Apr 16 is : 1970-01-01\n"
     ]
    }
   ],
   "source": [
    "#show min and max dates\n",
    "print(\"min arrdate i94 dataset from Apr 16 is : {}\".format(df_spark_new.agg({'arrdate': 'min'}).collect()[0][0]))\n",
    "print(\"max arrdate i94 dataset from Apr 16 is : {}\".format(df_spark_new.agg({'arrdate': 'max'}).collect()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min depdate i94 dataset from Apr 16 is : 1970-01-01\n",
      "max depdate i94 dataset from Apr 16 is : 1970-01-01\n"
     ]
    }
   ],
   "source": [
    "#show min and max dates\n",
    "print(\"min depdate i94 dataset from Apr 16 is : {}\".format(df_spark_new.agg({'depdate': 'min'}).collect()[0][0]))\n",
    "print(\"max depdate i94 dataset from Apr 16 is : {}\".format(df_spark_new.agg({'depdate': 'max'}).collect()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min i94yr i94 dataset from Apr 16 is : 2016\n",
      "max i94yr i94 dataset from Apr 16 is : 2016\n"
     ]
    }
   ],
   "source": [
    "#show min and max dates\n",
    "print(\"min i94yr i94 dataset from Apr 16 is : {}\".format(df_spark_new.agg({'i94yr': 'min'}).collect()[0][0]))\n",
    "print(\"max i94yr i94 dataset from Apr 16 is : {}\".format(df_spark_new.agg({'i94yr': 'max'}).collect()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min dtadfile i94 dataset from Apr 16 is : 2013-08-11\n",
      "max dtadfile i94 dataset from Apr 16 is : 2016-09-19\n"
     ]
    }
   ],
   "source": [
    "#show min and max dates\n",
    "print(\"min dtadfile i94 dataset from Apr 16 is : {}\".format(df_spark_new.agg({'dtadfile': 'min'}).collect()[0][0]))\n",
    "print(\"max dtadfile i94 dataset from Apr 16 is : {}\".format(df_spark_new.agg({'dtadfile': 'max'}).collect()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|i94port| Count|\n",
      "+-------+------+\n",
      "|    NYC|485916|\n",
      "|    MIA|343941|\n",
      "|    LOS|310163|\n",
      "|    SFR|152586|\n",
      "|    ORL|149195|\n",
      "|    HHW|142720|\n",
      "|    NEW|136122|\n",
      "|    CHI|130564|\n",
      "|    HOU|101481|\n",
      "|    FTL| 95977|\n",
      "|    ATL| 92579|\n",
      "|    LVG| 89280|\n",
      "|    AGA| 80919|\n",
      "|    WAS| 74835|\n",
      "|    DAL| 71809|\n",
      "|    BOS| 57354|\n",
      "|    SEA| 47719|\n",
      "|    PHO| 38890|\n",
      "|    DET| 37832|\n",
      "|    TAM| 25632|\n",
      "+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_new.groupBy(\"i94port\").agg(count(\"*\").alias('Count')).sort('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|visatype|  Count|\n",
      "+--------+-------+\n",
      "|      WT|1309059|\n",
      "|      B2|1117897|\n",
      "|      WB| 282983|\n",
      "|      B1| 212410|\n",
      "|     GMT|  89133|\n",
      "|      F1|  39016|\n",
      "|      E2|  19383|\n",
      "|      CP|  14758|\n",
      "|      E1|   3743|\n",
      "|       I|   3176|\n",
      "|      F2|   2984|\n",
      "|      M1|   1317|\n",
      "|      I1|    234|\n",
      "|     GMB|    150|\n",
      "|      M2|     49|\n",
      "|     SBP|     11|\n",
      "|     CPL|     10|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_new.groupBy(\"visatype\").agg(count(\"*\").alias('Count')).sort('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### From above data the top 5 Visa cateogeries under which people are travelling are\n",
    "  1. WT - waiver-tourist\n",
    "  2. B2 - visitor visa\n",
    "  3. WB - waiver- business\n",
    "  4. B1 - business visa\n",
    "  5. GMT - Global Marine Travel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Read demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#assign us cities demographic data csv file to a variable\n",
    "fname_demo = 'us-cities-demographics.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demo = spark.read.format('csv').options(header='true', inferschema='true', sep=';').load(fname_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|            City|         State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race| Count|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|   Silver Spring|      Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino| 25924|\n",
      "|          Quincy| Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White| 58723|\n",
      "|          Hoover|       Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian|  4759|\n",
      "|Rancho Cucamonga|    California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...| 24437|\n",
      "|          Newark|    New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White| 76402|\n",
      "|          Peoria|      Illinois|      33.1|          56229|            62432|          118661|              6634|        7517|                   2.4|        IL|American Indian a...|  1343|\n",
      "|        Avondale|       Arizona|      29.1|          38712|            41971|           80683|              4815|        8355|                  3.18|        AZ|Black or African-...| 11592|\n",
      "|     West Covina|    California|      39.8|          51629|            56860|          108489|              3800|       37038|                  3.56|        CA|               Asian| 32716|\n",
      "|        O'Fallon|      Missouri|      36.0|          41762|            43270|           85032|              5783|        3269|                  2.77|        MO|  Hispanic or Latino|  2583|\n",
      "|      High Point|North Carolina|      35.5|          51751|            58077|          109828|              5204|       16315|                  2.65|        NC|               Asian| 11060|\n",
      "|          Folsom|    California|      40.9|          41051|            35317|           76368|              4187|       13234|                  2.62|        CA|  Hispanic or Latino|  5822|\n",
      "|          Folsom|    California|      40.9|          41051|            35317|           76368|              4187|       13234|                  2.62|        CA|American Indian a...|   998|\n",
      "|    Philadelphia|  Pennsylvania|      34.1|         741270|           826172|         1567442|             61995|      205339|                  2.61|        PA|               Asian|122721|\n",
      "|         Wichita|        Kansas|      34.6|         192354|           197601|          389955|             23978|       40270|                  2.56|        KS|  Hispanic or Latino| 65162|\n",
      "|         Wichita|        Kansas|      34.6|         192354|           197601|          389955|             23978|       40270|                  2.56|        KS|American Indian a...|  8791|\n",
      "|      Fort Myers|       Florida|      37.3|          36850|            37165|           74015|              4312|       15365|                  2.45|        FL|               White| 50169|\n",
      "|      Pittsburgh|  Pennsylvania|      32.9|         149690|           154695|          304385|             17728|       28187|                  2.13|        PA|               White|208863|\n",
      "|          Laredo|         Texas|      28.8|         124305|           131484|          255789|              4921|       68427|                  3.66|        TX|American Indian a...|  1253|\n",
      "|        Berkeley|    California|      32.5|          60142|            60829|          120971|              3736|       25000|                  2.35|        CA|               Asian| 27089|\n",
      "|     Santa Clara|    California|      35.2|          63278|            62938|          126216|              4426|       52281|                  2.75|        CA|               White| 55847|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: double (nullable = true)\n",
      " |-- Male Population: integer (nullable = true)\n",
      " |-- Female Population: integer (nullable = true)\n",
      " |-- Total Population: integer (nullable = true)\n",
      " |-- Number of Veterans: integer (nullable = true)\n",
      " |-- Foreign-born: integer (nullable = true)\n",
      " |-- Average Household Size: double (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demo.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of records in US cities demographic dataset  is : 2891\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial number of records in US cities demographic dataset  is : {}\".format(df_demo.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Average Household Size</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Number of Veterans</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Foreign-born</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male Population</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Female Population</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>City</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Median Age</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Population</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State Code</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Race</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        count\n",
       "Average Household Size     16\n",
       "Number of Veterans         13\n",
       "Foreign-born               13\n",
       "Male Population             3\n",
       "Female Population           3\n",
       "City                        0\n",
       "State                       0\n",
       "Median Age                  0\n",
       "Total Population            0\n",
       "State Code                  0\n",
       "Race                        0\n",
       "Count                       0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count number of nulls in each of the columns\n",
    "count_missings(df_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+---------+-----------+\n",
      "|         city|Total Population|MaleCount|FemaleCount|\n",
      "+-------------+----------------+---------+-----------+\n",
      "|     New York|         8550405| 20408490|   22343535|\n",
      "|  Los Angeles|         3971896|  9794990|   10064490|\n",
      "|      Chicago|         2720556|  6600075|    7002705|\n",
      "|      Houston|         2298628|  5748430|    5744710|\n",
      "| Philadelphia|         1567442|  3706350|    4130860|\n",
      "|      Phoenix|         1563001|  3934165|    3880840|\n",
      "|  San Antonio|         1469824|  3607025|    3742095|\n",
      "|    San Diego|         1394907|  3469130|    3505405|\n",
      "|       Dallas|         1300082|  3195095|    3305315|\n",
      "|     San Jose|         1026919|  2591585|    2543010|\n",
      "|       Austin|          931840|  2378590|    2280610|\n",
      "| Jacksonville|          868031|  2096015|    2244140|\n",
      "|San Francisco|          864816|  2198760|    2125320|\n",
      "|     Columbus|          849067|  2069905|    2175430|\n",
      "| Indianapolis|          848423|  2053075|    2189040|\n",
      "|   Fort Worth|          836969|  2070630|    2114215|\n",
      "|    Charlotte|          827121|  1983230|    2152375|\n",
      "|      Seattle|          684443|  1728295|    1693920|\n",
      "|       Denver|          682545|  1705685|    1707040|\n",
      "|      El Paso|          681136|  1663985|    1741695|\n",
      "+-------------+----------------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demo.groupBy(\"city\",\"Total Population\").agg(F.sum(\"Male Population\").alias('MaleCount'),F.sum(\"Female Population\").alias('FemaleCount')).sort('Total Population', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|       city|                Race|\n",
      "+-----------+--------------------+\n",
      "|   New York|               White|\n",
      "|   New York|Black or African-...|\n",
      "|   New York|               Asian|\n",
      "|   New York|  Hispanic or Latino|\n",
      "|   New York|American Indian a...|\n",
      "|Los Angeles|  Hispanic or Latino|\n",
      "|Los Angeles|Black or African-...|\n",
      "|Los Angeles|               Asian|\n",
      "|Los Angeles|American Indian a...|\n",
      "|Los Angeles|               White|\n",
      "|    Chicago|               White|\n",
      "|    Chicago|Black or African-...|\n",
      "|    Chicago|American Indian a...|\n",
      "|    Chicago|               Asian|\n",
      "|    Chicago|  Hispanic or Latino|\n",
      "|    Houston|               White|\n",
      "|    Houston|               Asian|\n",
      "|    Houston|Black or African-...|\n",
      "|    Houston|  Hispanic or Latino|\n",
      "|    Houston|American Indian a...|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demo.groupBy(\"city\",\"Race\",\"Total Population\").agg(count(\"*\")).sort('Total Population', ascending=False).drop(\"count(1)\",\"Total Population\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Read Global Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fname_temp = '../../data2/GlobalLandTemperaturesByCity.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temp = spark.read.format('csv').options(header='true', inferschema='true', sep=',').load(fname_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|                 dt| AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+-------------------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01 00:00:00|              6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01 00:00:00|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01 00:00:00|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01 00:00:00|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01 00:00:00|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-04-01 00:00:00| 5.7879999999999985|           3.6239999999999997|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-05-01 00:00:00|             10.644|           1.2830000000000001|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-06-01 00:00:00| 14.050999999999998|                        1.347|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-07-01 00:00:00|             16.082|                        1.396|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-08-01 00:00:00|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-09-01 00:00:00| 12.780999999999999|                        1.454|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-10-01 00:00:00|               7.95|                         1.63|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-11-01 00:00:00|  4.638999999999999|           1.3019999999999998|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-12-01 00:00:00|0.12199999999999987|                        1.756|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-01-01 00:00:00|-1.3330000000000002|                        1.642|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-02-01 00:00:00|             -2.732|                        1.358|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-03-01 00:00:00|              0.129|                        1.088|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-04-01 00:00:00|              4.042|                        1.138|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-05-01 00:00:00|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-06-01 00:00:00|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+-------------------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#the defined function count_missings requires the columns to be string, so type casting the column to string\n",
    "changedTypedf = df_temp.withColumn(\"dt\", col(\"dt\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AverageTemperature</th>\n",
       "      <td>364130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <td>364130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>City</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latitude</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Longitude</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                count\n",
       "AverageTemperature             364130\n",
       "AverageTemperatureUncertainty  364130\n",
       "dt                                  0\n",
       "City                                0\n",
       "Country                             0\n",
       "Latitude                            0\n",
       "Longitude                           0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count number of nulls in each of the columns\n",
    "count_missings(changedTypedf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of records in global temparture dataset  is : 8599212\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial number of records in global temparture dataset  is : {}\".format(df_temp.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Read Airport Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fname_airport = 'airport-codes_csv.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport = spark.read.format('csv').options(header='true', inferschema='true', sep=',').load(fname_airport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "| 00AS|small_airport|      Fulton Airport|        1100|       NA|         US|     US-OK|        Alex|    00AS|     null|      00AS|-97.8180194, 34.9...|\n",
      "| 00AZ|small_airport|      Cordes Airport|        3810|       NA|         US|     US-AZ|      Cordes|    00AZ|     null|      00AZ|-112.165000915527...|\n",
      "| 00CA|small_airport|Goldstone /Gts/ A...|        3038|       NA|         US|     US-CA|     Barstow|    00CA|     null|      00CA|-116.888000488, 3...|\n",
      "| 00CL|small_airport| Williams Ag Airport|          87|       NA|         US|     US-CA|       Biggs|    00CL|     null|      00CL|-121.763427, 39.4...|\n",
      "| 00CN|     heliport|Kitchen Creek Hel...|        3350|       NA|         US|     US-CA| Pine Valley|    00CN|     null|      00CN|-116.4597417, 32....|\n",
      "| 00CO|       closed|          Cass Field|        4830|       NA|         US|     US-CO|  Briggsdale|    null|     null|      null|-104.344002, 40.6...|\n",
      "| 00FA|small_airport| Grass Patch Airport|          53|       NA|         US|     US-FL|    Bushnell|    00FA|     null|      00FA|-82.2190017700195...|\n",
      "| 00FD|     heliport|  Ringhaver Heliport|          25|       NA|         US|     US-FL|   Riverview|    00FD|     null|      00FD|-82.3453979492187...|\n",
      "| 00FL|small_airport|   River Oak Airport|          35|       NA|         US|     US-FL|  Okeechobee|    00FL|     null|      00FL|-80.9692001342773...|\n",
      "| 00GA|small_airport|    Lt World Airport|         700|       NA|         US|     US-GA|    Lithonia|    00GA|     null|      00GA|-84.0682983398437...|\n",
      "| 00GE|     heliport|    Caffrey Heliport|         957|       NA|         US|     US-GA|       Hiram|    00GE|     null|      00GE|-84.7339019775390...|\n",
      "| 00HI|     heliport|  Kaupulehu Heliport|          43|       NA|         US|     US-HI| Kailua/Kona|    00HI|     null|      00HI|-155.980233, 19.8...|\n",
      "| 00ID|small_airport|Delta Shores Airport|        2064|       NA|         US|     US-ID|  Clark Fork|    00ID|     null|      00ID|-116.213996887207...|\n",
      "| 00IG|small_airport|       Goltl Airport|        3359|       NA|         US|     US-KS|    McDonald|    00IG|     null|      00IG|-101.395994, 39.7...|\n",
      "| 00II|     heliport|Bailey Generation...|         600|       NA|         US|     US-IN|  Chesterton|    00II|     null|      00II|-87.122802734375,...|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>iata_code</th>\n",
       "      <td>45886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local_code</th>\n",
       "      <td>26389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gps_code</th>\n",
       "      <td>14045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elevation_ft</th>\n",
       "      <td>7006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>municipality</th>\n",
       "      <td>5676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ident</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>continent</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iso_country</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iso_region</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coordinates</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count\n",
       "iata_code     45886\n",
       "local_code    26389\n",
       "gps_code      14045\n",
       "elevation_ft   7006\n",
       "municipality   5676\n",
       "ident             0\n",
       "type              0\n",
       "name              0\n",
       "continent         0\n",
       "iso_country       0\n",
       "iso_region        0\n",
       "coordinates       0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count number of nulls in each of the columns\n",
    "count_missings(df_airport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Read i94port.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fname_port = 'i94port.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_port = spark.read.format('csv').options(header='false', inferschema='false', sep='=').load(fname_port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Clean the data by removing the enclosing quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_port = df_port.withColumn('port_code',F.regexp_replace('_c0', \"'\", ''))\\\n",
    "       .withColumn('port_name',F.regexp_replace('_c1', \"'\", ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Drop the default columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_port = df_port.drop('_c0','_c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- port_code: string (nullable = true)\n",
      " |-- port_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_port.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------------------+\n",
      "|port_code|port_name                         |\n",
      "+---------+----------------------------------+\n",
      "|   ALC\t  |\tALCAN, AK                        |\n",
      "|   ANC\t  |\tANCHORAGE, AK                    |\n",
      "|   BAR\t  |\tBAKER AAF - BAKER ISLAND, AK     |\n",
      "|   DAC\t  |\tDALTONS CACHE, AK                |\n",
      "|   PIZ\t  |\tDEW STATION PT LAY DEW, AK       |\n",
      "|   DTH\t  |\tDUTCH HARBOR, AK                 |\n",
      "|   EGL\t  |\tEAGLE, AK                        |\n",
      "|   FRB\t  |\tFAIRBANKS, AK                    |\n",
      "|   HOM\t  |\tHOMER, AK                        |\n",
      "|   HYD\t  |\tHYDER, AK                        |\n",
      "|   JUN\t  |\tJUNEAU, AK                       |\n",
      "|   5KE\t  |\tKETCHIKAN, AK                    |\n",
      "|   KET\t  |\tKETCHIKAN, AK                    |\n",
      "|   MOS\t  |\tMOSES POINT INTERMEDIATE, AK     |\n",
      "|   NIK\t  |\tNIKISKI, AK                      |\n",
      "|   NOM\t  |\tNOM, AK                          |\n",
      "|   PKC\t  |\tPOKER CREEK, AK                  |\n",
      "|   ORI\t  |\tPORT LIONS SPB, AK               |\n",
      "|   SKA\t  |\tSKAGWAY, AK                      |\n",
      "|   SNP\t  |\tST. PAUL ISLAND, AK              |\n",
      "+---------+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_port.show(truncate =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Read i94cntyl.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fname_cntyl = 'i94cntyl.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_cntyl = spark.read.format('csv').options(header='false', inferschema='false', sep='=').load(fname_cntyl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_cntyl = df_cntyl.withColumn('code',F.regexp_replace('_c0', \"'\", ''))\\\n",
    "       .withColumn('country',F.regexp_replace('_c1', \"'\", ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_cntyl = df_cntyl.drop('_c0','_c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cntyl.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------------------------------+\n",
      "|code   |country                                                    |\n",
      "+-------+-----------------------------------------------------------+\n",
      "|   582 |  MEXICO Air Sea, and Not Reported (I-94, no land arrivals)|\n",
      "|   236 |  AFGHANISTAN                                              |\n",
      "|   101 |  ALBANIA                                                  |\n",
      "|   316 |  ALGERIA                                                  |\n",
      "|   102 |  ANDORRA                                                  |\n",
      "|   324 |  ANGOLA                                                   |\n",
      "|   529 |  ANGUILLA                                                 |\n",
      "|   518 |  ANTIGUA-BARBUDA                                          |\n",
      "|   687 |  ARGENTINA                                                |\n",
      "|   151 |  ARMENIA                                                  |\n",
      "|   532 |  ARUBA                                                    |\n",
      "|   438 |  AUSTRALIA                                                |\n",
      "|   103 |  AUSTRIA                                                  |\n",
      "|   152 |  AZERBAIJAN                                               |\n",
      "|   512 |  BAHAMAS                                                  |\n",
      "|   298 |  BAHRAIN                                                  |\n",
      "|   274 |  BANGLADESH                                               |\n",
      "|   513 |  BARBADOS                                                 |\n",
      "|   104 |  BELGIUM                                                  |\n",
      "|   581 |  BELIZE                                                   |\n",
      "+-------+-----------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cntyl.show(truncate =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Steps\n",
    "Remove the insignificant columns and remove NAN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create dimension table dim_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Filter out the records where country value is either Invalid,or Collapsed or NoCountry Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_cntyl.filter(df_cntyl[\"country\"].rlike(\"(?!INVALID:)|(?!Collapsed)|(?!No Country Code)\")).createOrReplaceTempView(\"dim_countries\")\n",
    "dim_countries = spark.sql(\"\"\"\n",
    "                          SELECT CODE\n",
    "                                ,COUNTRY\n",
    "                          FROM DIM_COUNTRIES                  \n",
    "                         \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_countries.write.mode('overwrite').parquet('dim_countries.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------------------------------+\n",
      "|CODE   |COUNTRY                                                    |\n",
      "+-------+-----------------------------------------------------------+\n",
      "|   582 |  MEXICO Air Sea, and Not Reported (I-94, no land arrivals)|\n",
      "|   236 |  AFGHANISTAN                                              |\n",
      "|   101 |  ALBANIA                                                  |\n",
      "|   316 |  ALGERIA                                                  |\n",
      "|   102 |  ANDORRA                                                  |\n",
      "|   324 |  ANGOLA                                                   |\n",
      "|   529 |  ANGUILLA                                                 |\n",
      "|   518 |  ANTIGUA-BARBUDA                                          |\n",
      "|   687 |  ARGENTINA                                                |\n",
      "|   151 |  ARMENIA                                                  |\n",
      "|   532 |  ARUBA                                                    |\n",
      "|   438 |  AUSTRALIA                                                |\n",
      "|   103 |  AUSTRIA                                                  |\n",
      "|   152 |  AZERBAIJAN                                               |\n",
      "|   512 |  BAHAMAS                                                  |\n",
      "|   298 |  BAHRAIN                                                  |\n",
      "|   274 |  BANGLADESH                                               |\n",
      "|   513 |  BARBADOS                                                 |\n",
      "|   104 |  BELGIUM                                                  |\n",
      "|   581 |  BELIZE                                                   |\n",
      "+-------+-----------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_countries.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CODE: string (nullable = true)\n",
      " |-- COUNTRY: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_countries.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create dimension table dim_ports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Filter out the records for which either port code is not available or its value is Collapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_port.filter(df_port[\"port_code\"].rlike(\"(?!Collapsed)|(?!No PORT Code)\")).createOrReplaceTempView(\"dim_ports\")\n",
    "dim_ports = spark.sql(\"\"\"\n",
    "                          SELECT port_code\n",
    "                                ,port_name\n",
    "                          FROM DIM_PORTS                  \n",
    "                         \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_ports.write.mode('overwrite').parquet('dim_ports.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------------------+\n",
      "|port_code|port_name                         |\n",
      "+---------+----------------------------------+\n",
      "|   ALC\t  |\tALCAN, AK                        |\n",
      "|   ANC\t  |\tANCHORAGE, AK                    |\n",
      "|   BAR\t  |\tBAKER AAF - BAKER ISLAND, AK     |\n",
      "|   DAC\t  |\tDALTONS CACHE, AK                |\n",
      "|   PIZ\t  |\tDEW STATION PT LAY DEW, AK       |\n",
      "|   DTH\t  |\tDUTCH HARBOR, AK                 |\n",
      "|   EGL\t  |\tEAGLE, AK                        |\n",
      "|   FRB\t  |\tFAIRBANKS, AK                    |\n",
      "|   HOM\t  |\tHOMER, AK                        |\n",
      "|   HYD\t  |\tHYDER, AK                        |\n",
      "|   JUN\t  |\tJUNEAU, AK                       |\n",
      "|   5KE\t  |\tKETCHIKAN, AK                    |\n",
      "|   KET\t  |\tKETCHIKAN, AK                    |\n",
      "|   MOS\t  |\tMOSES POINT INTERMEDIATE, AK     |\n",
      "|   NIK\t  |\tNIKISKI, AK                      |\n",
      "|   NOM\t  |\tNOM, AK                          |\n",
      "|   PKC\t  |\tPOKER CREEK, AK                  |\n",
      "|   ORI\t  |\tPORT LIONS SPB, AK               |\n",
      "|   SKA\t  |\tSKAGWAY, AK                      |\n",
      "|   SNP\t  |\tST. PAUL ISLAND, AK              |\n",
      "+---------+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_ports.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- port_code: string (nullable = true)\n",
      " |-- port_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_ports.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create dimension dim_temparature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Remove records where AverageTemperature is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Remove null values from global temparature dataset\n",
    "\n",
    "df_temp_cleaned = df_temp.filter(df_temp.AverageTemperature.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in global temparture dataset after removing nulls is : 8235082\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of records in global temparture dataset after removing nulls is : {}\".format(df_temp_cleaned.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temp_cleaned.createOrReplaceTempView(\"dim_temparature\")\n",
    "dim_temparature = spark.sql(\"\"\"\n",
    "                                SELECT dt as date\n",
    "                                      ,Country\n",
    "                                      ,City\n",
    "                                      ,AverageTemperature\n",
    "                                      ,AverageTemperatureUncertainty\n",
    "                                      ,Latitude\n",
    "                                      ,Longitude\n",
    "                                FROM DIM_TEMPARATURE                  \n",
    "                            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_temparature.write.mode('overwrite').parquet('dim_temparature.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-----+-------------------+-----------------------------+--------+---------+\n",
      "|date               |Country|City |AverageTemperature |AverageTemperatureUncertainty|Latitude|Longitude|\n",
      "+-------------------+-------+-----+-------------------+-----------------------------+--------+---------+\n",
      "|1743-11-01 00:00:00|Denmark|Århus|6.068              |1.7369999999999999           |57.05N  |10.33E   |\n",
      "|1744-04-01 00:00:00|Denmark|Århus|5.7879999999999985 |3.6239999999999997           |57.05N  |10.33E   |\n",
      "|1744-05-01 00:00:00|Denmark|Århus|10.644             |1.2830000000000001           |57.05N  |10.33E   |\n",
      "|1744-06-01 00:00:00|Denmark|Århus|14.050999999999998 |1.347                        |57.05N  |10.33E   |\n",
      "|1744-07-01 00:00:00|Denmark|Århus|16.082             |1.396                        |57.05N  |10.33E   |\n",
      "|1744-09-01 00:00:00|Denmark|Århus|12.780999999999999 |1.454                        |57.05N  |10.33E   |\n",
      "|1744-10-01 00:00:00|Denmark|Århus|7.95               |1.63                         |57.05N  |10.33E   |\n",
      "|1744-11-01 00:00:00|Denmark|Århus|4.638999999999999  |1.3019999999999998           |57.05N  |10.33E   |\n",
      "|1744-12-01 00:00:00|Denmark|Århus|0.12199999999999987|1.756                        |57.05N  |10.33E   |\n",
      "|1745-01-01 00:00:00|Denmark|Århus|-1.3330000000000002|1.642                        |57.05N  |10.33E   |\n",
      "|1745-02-01 00:00:00|Denmark|Århus|-2.732             |1.358                        |57.05N  |10.33E   |\n",
      "|1745-03-01 00:00:00|Denmark|Århus|0.129              |1.088                        |57.05N  |10.33E   |\n",
      "|1745-04-01 00:00:00|Denmark|Århus|4.042              |1.138                        |57.05N  |10.33E   |\n",
      "|1750-01-01 00:00:00|Denmark|Århus|1.699              |1.013                        |57.05N  |10.33E   |\n",
      "|1750-02-01 00:00:00|Denmark|Århus|3.9610000000000003 |2.3609999999999998           |57.05N  |10.33E   |\n",
      "|1750-03-01 00:00:00|Denmark|Århus|5.182              |3.48                         |57.05N  |10.33E   |\n",
      "|1750-04-01 00:00:00|Denmark|Århus|7.197              |0.732                        |57.05N  |10.33E   |\n",
      "|1750-05-01 00:00:00|Denmark|Århus|10.634             |1.351                        |57.05N  |10.33E   |\n",
      "|1750-06-01 00:00:00|Denmark|Århus|14.913             |1.181                        |57.05N  |10.33E   |\n",
      "|1750-07-01 00:00:00|Denmark|Århus|17.831             |1.22                         |57.05N  |10.33E   |\n",
      "+-------------------+-------+-----+-------------------+-----------------------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_temparature.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create dimension dim_demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demo.createOrReplaceTempView(\"dim_demographics\")\n",
    "dim_demographics = spark.sql(\"\"\"\n",
    "                                SELECT \"State Code\" as StateCode\n",
    "                                      ,State\n",
    "                                      ,City\n",
    "                                      ,\"Total Population\" as TotalPopulation\n",
    "                                      ,\"Male Population\" as MalePopulation\n",
    "                                      ,\"Female Population\" as FemalePopulation\n",
    "                                      ,\"Median Age\"as MedianAge\n",
    "                                      ,\"Average Household Size\" as AvgHouseholdSize\n",
    "                                      ,\"Number of Veterans\" as NumVeterans\n",
    "                                      ,\"Foreign-born\" as ForeignBorn\n",
    "                                      ,Race\n",
    "                                FROM DIM_DEMOGRAPHICS                  \n",
    "                            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_demographics.write.mode('overwrite').parquet('dim_demographics.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+----------------+----------------+---------------+-----------------+----------+----------------------+------------------+------------+---------------------------------+\n",
      "|StateCode |State         |City            |TotalPopulation |MalePopulation |FemalePopulation |MedianAge |AvgHouseholdSize      |NumVeterans       |ForeignBorn |Race                             |\n",
      "+----------+--------------+----------------+----------------+---------------+-----------------+----------+----------------------+------------------+------------+---------------------------------+\n",
      "|State Code|Maryland      |Silver Spring   |Total Population|Male Population|Female Population|Median Age|Average Household Size|Number of Veterans|Foreign-born|Hispanic or Latino               |\n",
      "|State Code|Massachusetts |Quincy          |Total Population|Male Population|Female Population|Median Age|Average Household Size|Number of Veterans|Foreign-born|White                            |\n",
      "|State Code|Alabama       |Hoover          |Total Population|Male Population|Female Population|Median Age|Average Household Size|Number of Veterans|Foreign-born|Asian                            |\n",
      "|State Code|California    |Rancho Cucamonga|Total Population|Male Population|Female Population|Median Age|Average Household Size|Number of Veterans|Foreign-born|Black or African-American        |\n",
      "|State Code|New Jersey    |Newark          |Total Population|Male Population|Female Population|Median Age|Average Household Size|Number of Veterans|Foreign-born|White                            |\n",
      "|State Code|Illinois      |Peoria          |Total Population|Male Population|Female Population|Median Age|Average Household Size|Number of Veterans|Foreign-born|American Indian and Alaska Native|\n",
      "|State Code|Arizona       |Avondale        |Total Population|Male Population|Female Population|Median Age|Average Household Size|Number of Veterans|Foreign-born|Black or African-American        |\n",
      "|State Code|California    |West Covina     |Total Population|Male Population|Female Population|Median Age|Average Household Size|Number of Veterans|Foreign-born|Asian                            |\n",
      "|State Code|Missouri      |O'Fallon        |Total Population|Male Population|Female Population|Median Age|Average Household Size|Number of Veterans|Foreign-born|Hispanic or Latino               |\n",
      "|State Code|North Carolina|High Point      |Total Population|Male Population|Female Population|Median Age|Average Household Size|Number of Veterans|Foreign-born|Asian                            |\n",
      "|State Code|California    |Folsom          |Total Population|Male Population|Female Population|Median Age|Average Household Size|Number of Veterans|Foreign-born|Hispanic or Latino               |\n",
      "|State Code|California    |Folsom          |Total Population|Male Population|Female Population|Median Age|Average Household Size|Number of Veterans|Foreign-born|American Indian and Alaska Native|\n",
      "|State Code|Pennsylvania  |Philadelphia    |Total Population|Male Population|Female Population|Median Age|Average Household Size|Number of Veterans|Foreign-born|Asian                            |\n",
      "|State Code|Kansas        |Wichita         |Total Population|Male Population|Female Population|Median Age|Average Household Size|Number of Veterans|Foreign-born|Hispanic or Latino               |\n",
      "|State Code|Kansas        |Wichita         |Total Population|Male Population|Female Population|Median Age|Average Household Size|Number of Veterans|Foreign-born|American Indian and Alaska Native|\n",
      "|State Code|Florida       |Fort Myers      |Total Population|Male Population|Female Population|Median Age|Average Household Size|Number of Veterans|Foreign-born|White                            |\n",
      "|State Code|Pennsylvania  |Pittsburgh      |Total Population|Male Population|Female Population|Median Age|Average Household Size|Number of Veterans|Foreign-born|White                            |\n",
      "|State Code|Texas         |Laredo          |Total Population|Male Population|Female Population|Median Age|Average Household Size|Number of Veterans|Foreign-born|American Indian and Alaska Native|\n",
      "|State Code|California    |Berkeley        |Total Population|Male Population|Female Population|Median Age|Average Household Size|Number of Veterans|Foreign-born|Asian                            |\n",
      "|State Code|California    |Santa Clara     |Total Population|Male Population|Female Population|Median Age|Average Household Size|Number of Veterans|Foreign-born|White                            |\n",
      "+----------+--------------+----------------+----------------+---------------+-----------------+----------+----------------------+------------------+------------+---------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_demographics.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- StateCode: string (nullable = false)\n",
      " |-- State: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- TotalPopulation: string (nullable = false)\n",
      " |-- MalePopulation: string (nullable = false)\n",
      " |-- FemalePopulation: string (nullable = false)\n",
      " |-- MedianAge: string (nullable = false)\n",
      " |-- AvgHouseholdSize: string (nullable = false)\n",
      " |-- NumVeterans: string (nullable = false)\n",
      " |-- ForeignBorn: string (nullable = false)\n",
      " |-- Race: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_demographics.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create dimension dim_airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport.createOrReplaceTempView(\"dim_airports\")\n",
    "dim_airports  = spark.sql(\"\"\"\n",
    "                                SELECT   ident\n",
    "                                        ,type\n",
    "                                        ,name\n",
    "                                        ,elevation_ft\n",
    "                                        ,continent\n",
    "                                        ,iso_country\n",
    "                                        ,iso_region\n",
    "                                        ,municipality\n",
    "                                        ,gps_code\n",
    "                                        ,iata_code\n",
    "                                        ,local_code\n",
    "                                        ,coordinates\n",
    "                                FROM DIM_AIRPORTS                  \n",
    "                            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_airports.write.mode('overwrite').parquet('dim_airports.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+----------------------------------+------------+---------+-----------+----------+------------+--------+---------+----------+---------------------------------------+\n",
      "|ident|type         |name                              |elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|coordinates                            |\n",
      "+-----+-------------+----------------------------------+------------+---------+-----------+----------+------------+--------+---------+----------+---------------------------------------+\n",
      "|00A  |heliport     |Total Rf Heliport                 |11          |NA       |US         |US-PA     |Bensalem    |00A     |null     |00A       |-74.93360137939453, 40.07080078125     |\n",
      "|00AA |small_airport|Aero B Ranch Airport              |3435        |NA       |US         |US-KS     |Leoti       |00AA    |null     |00AA      |-101.473911, 38.704022                 |\n",
      "|00AK |small_airport|Lowell Field                      |450         |NA       |US         |US-AK     |Anchor Point|00AK    |null     |00AK      |-151.695999146, 59.94919968            |\n",
      "|00AL |small_airport|Epps Airpark                      |820         |NA       |US         |US-AL     |Harvest     |00AL    |null     |00AL      |-86.77030181884766, 34.86479949951172  |\n",
      "|00AR |closed       |Newport Hospital & Clinic Heliport|237         |NA       |US         |US-AR     |Newport     |null    |null     |null      |-91.254898, 35.6087                    |\n",
      "|00AS |small_airport|Fulton Airport                    |1100        |NA       |US         |US-OK     |Alex        |00AS    |null     |00AS      |-97.8180194, 34.9428028                |\n",
      "|00AZ |small_airport|Cordes Airport                    |3810        |NA       |US         |US-AZ     |Cordes      |00AZ    |null     |00AZ      |-112.16500091552734, 34.305599212646484|\n",
      "|00CA |small_airport|Goldstone /Gts/ Airport           |3038        |NA       |US         |US-CA     |Barstow     |00CA    |null     |00CA      |-116.888000488, 35.350498199499995     |\n",
      "|00CL |small_airport|Williams Ag Airport               |87          |NA       |US         |US-CA     |Biggs       |00CL    |null     |00CL      |-121.763427, 39.427188                 |\n",
      "|00CN |heliport     |Kitchen Creek Helibase Heliport   |3350        |NA       |US         |US-CA     |Pine Valley |00CN    |null     |00CN      |-116.4597417, 32.7273736               |\n",
      "|00CO |closed       |Cass Field                        |4830        |NA       |US         |US-CO     |Briggsdale  |null    |null     |null      |-104.344002, 40.622202                 |\n",
      "|00FA |small_airport|Grass Patch Airport               |53          |NA       |US         |US-FL     |Bushnell    |00FA    |null     |00FA      |-82.21900177001953, 28.64550018310547  |\n",
      "|00FD |heliport     |Ringhaver Heliport                |25          |NA       |US         |US-FL     |Riverview   |00FD    |null     |00FD      |-82.34539794921875, 28.846599578857422 |\n",
      "|00FL |small_airport|River Oak Airport                 |35          |NA       |US         |US-FL     |Okeechobee  |00FL    |null     |00FL      |-80.96920013427734, 27.230899810791016 |\n",
      "|00GA |small_airport|Lt World Airport                  |700         |NA       |US         |US-GA     |Lithonia    |00GA    |null     |00GA      |-84.06829833984375, 33.76750183105469  |\n",
      "|00GE |heliport     |Caffrey Heliport                  |957         |NA       |US         |US-GA     |Hiram       |00GE    |null     |00GE      |-84.73390197753906, 33.88420104980469  |\n",
      "|00HI |heliport     |Kaupulehu Heliport                |43          |NA       |US         |US-HI     |Kailua/Kona |00HI    |null     |00HI      |-155.980233, 19.832715                 |\n",
      "|00ID |small_airport|Delta Shores Airport              |2064        |NA       |US         |US-ID     |Clark Fork  |00ID    |null     |00ID      |-116.21399688720703, 48.145301818847656|\n",
      "|00IG |small_airport|Goltl Airport                     |3359        |NA       |US         |US-KS     |McDonald    |00IG    |null     |00IG      |-101.395994, 39.724028                 |\n",
      "|00II |heliport     |Bailey Generation Station Heliport|600         |NA       |US         |US-IN     |Chesterton  |00II    |null     |00II      |-87.122802734375, 41.644500732421875   |\n",
      "+-----+-------------+----------------------------------+------------+---------+-----------+----------+------------+--------+---------+----------+---------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_airports.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_airports.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create Fact  fact_immigration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Remove records from I94 data having state code values for which data is available in dim_ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark_new.createOrReplaceTempView(\"fact_immigration\")\n",
    "fact_immigration  = spark.sql(\"\"\"\n",
    "                                SELECT   *\n",
    "                                FROM FACT_IMMIGRATION\n",
    "                                WHERE i94port in  (select port_code from dim_ports)\n",
    "                            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark_new = df_spark_new.withColumnRenamed('i94port','port_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark_new = df_spark_new.join(dim_ports, [\"port_code\"], \"leftanti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark_new = df_spark_new.withColumn(\"skey\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create fact table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark_new.createOrReplaceTempView(\"fact_immigration\")\n",
    "fact_immigration = spark.sql(\"\"\"\n",
    "                                SELECT   skey \n",
    "                                        ,i94yr \n",
    "                                        ,i94mon  \n",
    "                                        ,i94cit\n",
    "                                        ,i94res\n",
    "                                        ,port_code  \n",
    "                                        ,arrdate  \n",
    "                                        ,i94mode  \n",
    "                                        ,i94addr  \n",
    "                                        ,depdate  \n",
    "                                        ,i94bir  \n",
    "                                        ,i94visa  \n",
    "                                        ,dtadfile  \n",
    "                                        ,visapost  \n",
    "                                        ,occup  \n",
    "                                        ,entdepa  \n",
    "                                        ,entdepd  \n",
    "                                        ,matflag  \n",
    "                                        ,biryear  \n",
    "                                        ,dtaddto  \n",
    "                                        ,gender  \n",
    "                                        ,airline  \n",
    "                                        ,visatype  \n",
    "                                FROM fact_immigration                  \n",
    "                            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_immigration.write.mode('overwrite').parquet('fact_immigration.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+------+------+---------+----------+-------+-------+----------+------+-------+----------+--------+-----+-------+-------+-------+-------+-------+------+-------+--------+\n",
      "|skey|i94yr|i94mon|i94cit|i94res|port_code|arrdate   |i94mode|i94addr|depdate   |i94bir|i94visa|dtadfile  |visapost|occup|entdepa|entdepd|matflag|biryear|dtaddto|gender|airline|visatype|\n",
      "+----+-----+------+------+------+---------+----------+-------+-------+----------+------+-------+----------+--------+-----+-------+-------+-------+-------+-------+------+-------+--------+\n",
      "|0   |2016 |4     |245.0 |438.0 |LOS      |1970-01-01|1.0    |CA     |1970-01-01|40.0  |1.0    |2016-04-30|SYD     |null |G      |O      |M      |1976   |null   |F     |QF     |B1      |\n",
      "|1   |2016 |4     |245.0 |438.0 |LOS      |1970-01-01|1.0    |NV     |1970-01-01|32.0  |1.0    |2016-04-30|SYD     |null |G      |O      |M      |1984   |null   |F     |VA     |B1      |\n",
      "|2   |2016 |4     |245.0 |438.0 |LOS      |1970-01-01|1.0    |WA     |1970-01-01|29.0  |1.0    |2016-04-30|SYD     |null |G      |O      |M      |1987   |null   |M     |DL     |B1      |\n",
      "|3   |2016 |4     |245.0 |438.0 |LOS      |1970-01-01|1.0    |WA     |1970-01-01|29.0  |1.0    |2016-04-30|SYD     |null |G      |O      |M      |1987   |null   |F     |DL     |B1      |\n",
      "|4   |2016 |4     |245.0 |438.0 |LOS      |1970-01-01|1.0    |WA     |1970-01-01|28.0  |1.0    |2016-04-30|SYD     |null |G      |O      |M      |1988   |null   |M     |DL     |B1      |\n",
      "|5   |2016 |4     |245.0 |464.0 |HHW      |1970-01-01|1.0    |HI     |1970-01-01|57.0  |2.0    |2016-04-30|ACK     |null |G      |O      |M      |1959   |null   |M     |NZ     |B2      |\n",
      "|6   |2016 |4     |245.0 |464.0 |HHW      |1970-01-01|1.0    |HI     |1970-01-01|66.0  |2.0    |2016-04-30|ACK     |null |G      |O      |M      |1950   |null   |F     |NZ     |B2      |\n",
      "|7   |2016 |4     |245.0 |464.0 |HHW      |1970-01-01|1.0    |HI     |1970-01-01|41.0  |2.0    |2016-04-30|ACK     |null |G      |O      |M      |1975   |null   |F     |NZ     |B2      |\n",
      "|8   |2016 |4     |245.0 |464.0 |HOU      |1970-01-01|1.0    |FL     |1970-01-01|27.0  |2.0    |2016-04-30|ACK     |null |G      |O      |M      |1989   |null   |M     |NZ     |B2      |\n",
      "|9   |2016 |4     |245.0 |464.0 |LOS      |1970-01-01|1.0    |CA     |1970-01-01|26.0  |2.0    |2016-04-30|ACK     |null |G      |O      |M      |1990   |null   |F     |NZ     |B2      |\n",
      "|10  |2016 |4     |245.0 |504.0 |NEW      |1970-01-01|1.0    |MA     |1970-01-01|44.0  |2.0    |2016-04-30|GUZ     |null |G      |O      |M      |1972   |null   |M     |UA     |B2      |\n",
      "|11  |2016 |4     |245.0 |504.0 |LOS      |1970-01-01|1.0    |null   |1970-01-01|39.0  |2.0    |2016-04-30|GUZ     |null |G      |O      |M      |1977   |null   |M     |CM     |B2      |\n",
      "|12  |2016 |4     |245.0 |504.0 |WAS      |1970-01-01|1.0    |VA     |1970-01-01|38.0  |2.0    |2016-04-30|PNM     |null |G      |O      |M      |1978   |null   |M     |CM     |B2      |\n",
      "|13  |2016 |4     |245.0 |504.0 |LOS      |1970-01-01|1.0    |CA     |1970-01-01|56.0  |2.0    |2016-04-30|PNM     |null |G      |O      |M      |1960   |null   |F     |CM     |B2      |\n",
      "|14  |2016 |4     |245.0 |504.0 |LOS      |1970-01-01|1.0    |CA     |1970-01-01|38.0  |2.0    |2016-04-30|PNM     |null |G      |O      |M      |1978   |null   |M     |CM     |B2      |\n",
      "|15  |2016 |4     |245.0 |504.0 |MIA      |1970-01-01|1.0    |FL     |1970-01-01|53.0  |2.0    |2016-04-30|PNM     |null |G      |O      |M      |1963   |null   |F     |CM     |B2      |\n",
      "|16  |2016 |4     |245.0 |528.0 |SFR      |1970-01-01|1.0    |CA     |null      |84.0  |2.0    |2016-04-30|HNK     |null |G      |null   |null   |1932   |null   |F     |CX     |B2      |\n",
      "|17  |2016 |4     |245.0 |582.0 |HOU      |1970-01-01|1.0    |TX     |1970-01-01|43.0  |1.0    |2016-04-30|GUZ     |null |G      |O      |M      |1973   |null   |M     |UA     |B1      |\n",
      "|18  |2016 |4     |245.0 |582.0 |HOU      |1970-01-01|1.0    |TX     |1970-01-01|30.0  |1.0    |2016-04-30|GUZ     |null |G      |O      |M      |1986   |null   |F     |UA     |B1      |\n",
      "|19  |2016 |4     |245.0 |582.0 |LOS      |1970-01-01|1.0    |CA     |1970-01-01|34.0  |2.0    |2016-04-30|SHG     |null |G      |O      |M      |1982   |null   |M     |AM     |B2      |\n",
      "+----+-----+------+------+------+---------+----------+-------+-------+----------+------+-------+----------+--------+-----+-------+-------+-------+-------+-------+------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_immigration.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- skey: long (nullable = false)\n",
      " |-- i94yr: integer (nullable = true)\n",
      " |-- i94mon: integer (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- port_code: string (nullable = true)\n",
      " |-- arrdate: string (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: string (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- dtadfile: date (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: integer (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_immigration.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "#### 3.1 Conceptual Data Model\n",
    "The ETL Pipeline consist of below dimenion and Fact tables. Star Schema is being used to model the data.\n",
    " * Dimension Tables  \n",
    "   * dim_countries  \n",
    "   * dim_ports\n",
    "   * dim_temparature  \n",
    "   * dim_demographics  \n",
    "   * dim_airports\n",
    " * Fact Table  \n",
    "   * fact_immigration  \n",
    " \n",
    "   \n",
    " * Conceptual Model (Since this is conceptual model, it does not show the Refrential Constraints)  \n",
    "   \n",
    " ![ConceptuaModel](https://r766466c839826xjupyterlnnfq3jud.udacity-student-workspaces.com/files/Conceptual%20Model.PNG?_xsrf=2%7Cd0db3c9c%7C35f88a0bf6691a0cba5aa87c43349937%7C1585202570)\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The ETL pipeline can be modeled using airflow hosted on AWS EC2 cluster. The data pipeline will look as below\n",
    "\n",
    "S3 will be used as a data source and to store the dimension tables in parquet file format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "create_dimension_tables >> load_dim_data_from_s3  \n",
    "create_fact_table  \n",
    "load_dim_data_from_s3 >> load_data_into_fact_table  \n",
    "load_data_into_fact_table >> data_validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records in fact table is : 3096313\n"
     ]
    }
   ],
   "source": [
    "#Fact table should not be empty\n",
    "print(\"Total number of records in fact table is : {}\".format(fact_immigration.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* I94 Immigration Data  \n",
    "  * I94YR - 4 digit year  \n",
    "  * I94MON - Numeric month  \n",
    "  * I94CIT - City where I94 was issued  \n",
    "  * port_code - valid ports for processing  \n",
    "  * ARRDATE - the Arrival Date in the USA.  \n",
    "  * I94MODE - Mode of travel to USA  \n",
    "  * I94ADDR -There is lots of invalid codes in this variable and the list below   \n",
    "         shows what we have found to be valid, everything else goes into 'other'  \n",
    "  * DEPDATE is the Departure Date from the USA. It is a SAS date numeric field that  \n",
    "    a permament format has not been applied.  Please apply whichever date format  \n",
    "    works for you. \n",
    "  * I94BIR - Age of Respondent in Years \n",
    "  * I94VISA - Visa codes collapsed into three categories:  \n",
    "    * 1 = Business\n",
    "    * 2 = Pleasure\n",
    "    * 3 = Student\n",
    "  * DTADFILE - Character Date Field - Date added to I-94 Files - CIC does not use \n",
    "  * VISAPOST - Department of State where where Visa was issued - CIC does not use \n",
    "  * OCCUP - Occupation that will be performed in U.S. - CIC does not use \n",
    "  * ENTDEPA - Arrival Flag - admitted or paroled into the U.S. - CIC does not use \n",
    "  * ENTDEPD - Departure Flag - Departed, lost I-94 or is deceased - CIC does not use \n",
    "  * ENTDEPU - Update Flag - Either apprehended, overstayed, adjusted to perm residence - CIC does not use \n",
    "  * MATFLAG - Match flag - Match of arrival and departure records \n",
    "  * BIRYEAR - 4 digit year of birth \n",
    "  * DTADDTO - Character Date Field - Date to which admitted to U.S. (allowed to stay until) - CIC does not use \n",
    "  * GENDER - Non-immigrant sex \n",
    "  * AIRLINE - Airline used to arrive in U.S. \n",
    "  * VISATYPE - Class of admission legally admitting the non-immigrant to temporarily stay in U.S.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    " * Pyspark isused to build the ETL code due to its efficieny in processing large volume of data.  \n",
    "   Pyspark can also process multiple formats of data efficiently.\n",
    "* Propose how often the data should be updated and why.  \n",
    "   Data should be updated based on the available subscription of US I94 data. \n",
    "   Most recent data will ensure that the reports are current and up to date metrics to consumers of the data.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.  \n",
    "   The ETL pipeline can be mored to a bigger EMR cluster in AWS using better instance types.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.  \n",
    "   Airflow DAGs can be scheduled to populate the dashboard by 7am every day.\n",
    " * The database needed to be accessed by 100+ people.  \n",
    "   Dimensiaonal model can be hosted in Redshift to handle the increasing workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
